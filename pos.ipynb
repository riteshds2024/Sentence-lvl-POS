{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ff2e22-fec3-46e5-b849-4aeb8343ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "C:\\Users\\rites\\AppData\\Local\\Temp\\ipykernel_6188\\2444071323.py:39: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dictionary_df = dictionary_df.applymap(lambda x: preprocess(x) if isinstance(x, str) else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text-ID</th>\n",
       "      <th>Sentence-ID</th>\n",
       "      <th>Security</th>\n",
       "      <th>Conformity</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Benevolence</th>\n",
       "      <th>Universalism</th>\n",
       "      <th>Self-Direction</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text-ID  Sentence-ID  Security  Conformity  Tradition  Benevolence  \\\n",
       "0  BG_002            1         0           0          0            0   \n",
       "1  BG_002            2         0           0          0            0   \n",
       "2  BG_002            3         0           1          0            0   \n",
       "3  BG_002            4         0           0          0            0   \n",
       "4  BG_002            5         0           0          0            0   \n",
       "\n",
       "   Universalism  Self-Direction  Stimulation  Hedonism  Achievement  Power  \n",
       "0             0               0            0         0            0      0  \n",
       "1             0               0            0         0            0      0  \n",
       "2             0               0            0         0            0      2  \n",
       "3             0               0            0         0            0      0  \n",
       "4             2               0            0         1            0      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load dictionary and test data\n",
    "dictionary_path = 'Dictionary 10.xlsx'\n",
    "test_data_path = 'TEST DATA.csv'\n",
    "\n",
    "# Read dictionary from excel file\n",
    "dictionary_df = pd.read_excel(dictionary_path)\n",
    "\n",
    "# Read test data from csv file\n",
    "test_data_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess text: lowercase, remove stopwords, lemmatize, and POS tag\n",
    "def preprocess(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]\n",
    "    pos_words = pos_tag(words)\n",
    "    # Keep only specific POS tags (e.g., nouns, adjectives, and verbs)\n",
    "    allowed_pos = {'NN', 'JJ', 'VB'}\n",
    "    filtered_words = [word for word, pos in pos_words if pos in allowed_pos]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Preprocess dictionary words\n",
    "dictionary_df = dictionary_df.applymap(lambda x: preprocess(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Preprocess test data sentences\n",
    "test_data_df['Text'] = test_data_df['Text'].astype(str).apply(preprocess)\n",
    "\n",
    "# Initialize output dataframe with the necessary columns\n",
    "output_df = test_data_df[['Text-ID', 'Sentence-ID']].copy()\n",
    "\n",
    "# Initialize dictionary count columns with zeros\n",
    "behavior_columns = [\"Security\", \"Conformity\", \"Tradition\", \"Benevolence\", \"Universalism\",\n",
    "                    \"Self-Direction\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power\"]\n",
    "\n",
    "for column in behavior_columns:\n",
    "    output_df[column] = 0\n",
    "\n",
    "# Function to count matches\n",
    "def count_matches(sentence, words_list):\n",
    "    sentence_words = set(sentence.split())\n",
    "    return sum(word in sentence_words for word in words_list)\n",
    "\n",
    "# Update output dataframe with counts\n",
    "for idx, row in test_data_df.iterrows():\n",
    "    sentence = row['Text']  # Assuming the sentence is in the 'Text' column\n",
    "    for column in behavior_columns:\n",
    "        words_list = dictionary_df[column].dropna().tolist()\n",
    "        output_df.at[idx, column] = count_matches(sentence, words_list)\n",
    "\n",
    "# Select only the required columns for the output\n",
    "output_df = output_df[['Text-ID', 'Sentence-ID'] + behavior_columns]\n",
    "\n",
    "# Save the output dataframe to a new CSV file\n",
    "output_file_path = 'POS_Exact_Match_Output.csv'\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "output_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e51111d-9654-44d6-86b4-17ed7c8b3c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text-ID</th>\n",
       "      <th>Sentence-ID</th>\n",
       "      <th>Security</th>\n",
       "      <th>Conformity</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Benevolence</th>\n",
       "      <th>Universalism</th>\n",
       "      <th>Self-Direction</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text-ID  Sentence-ID  Security  Conformity  Tradition  Benevolence  \\\n",
       "0  BG_002            1         0           0          0            0   \n",
       "1  BG_002            2         0           0          0            0   \n",
       "2  BG_002            3         0           0          0            0   \n",
       "3  BG_002            4         0           0          0            0   \n",
       "4  BG_002            5         0           0          0            0   \n",
       "\n",
       "   Universalism  Self-Direction  Stimulation  Hedonism  Achievement  Power  \n",
       "0             0               0            0         0            0      0  \n",
       "1             0               0            0         0            0      0  \n",
       "2             0               0            0         0            0      1  \n",
       "3             0               0            0         0            0      0  \n",
       "4             1               0            0         0            0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the previously generated output file\n",
    "output_file_path = 'POS_Exact_Match_Output.csv'  # Update this path if necessary\n",
    "output_df = pd.read_csv(output_file_path)\n",
    "\n",
    "# Define the columns to process\n",
    "columns_to_process = [\"Self-Direction\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power\", \"Security\",\n",
    "                      \"Tradition\", \"Conformity\", \"Benevolence\", \"Universalism\"]\n",
    "\n",
    "# Iterate over each row\n",
    "for idx, row in output_df.iterrows():\n",
    "    # Check if all values are 0\n",
    "    if (row[columns_to_process] == 0).all():\n",
    "        continue  # Skip updating this row if all values are 0\n",
    "    \n",
    "    # Find the maximum value for the row\n",
    "    max_value = row[columns_to_process].max()\n",
    "    \n",
    "    # Update columns: set to 1 if equal to max value, else 0\n",
    "    for column in columns_to_process:\n",
    "        output_df.at[idx, column] = 1 if row[column] == max_value else 0\n",
    "\n",
    "# Save the updated output dataframe to a new CSV file\n",
    "updated_output_file_path = 'POS_Exact_Match_Output_upd.csv'  # Update this path if necessary\n",
    "output_df.to_csv(updated_output_file_path, index=False)\n",
    "\n",
    "output_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd88a9bc-0706-4c91-a6fe-7662554f0093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score Columns:\n",
      "Index(['Text-ID', 'Sentence-ID', 'Security', 'Conformity', 'Tradition',\n",
      "       'Benevolence', 'Universalism', 'Self-Direction', 'Stimulation',\n",
      "       'Hedonism', 'Achievement', 'Power'],\n",
      "      dtype='object')\n",
      "\n",
      "Updated Output Columns:\n",
      "Index(['Text-ID', 'Sentence-ID', 'Security', 'Conformity', 'Tradition',\n",
      "       'Benevolence', 'Universalism', 'Self-Direction', 'Stimulation',\n",
      "       'Hedonism', 'Achievement', 'Power'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text-ID</th>\n",
       "      <th>Sentence-ID</th>\n",
       "      <th>Security_test</th>\n",
       "      <th>Conformity_test</th>\n",
       "      <th>Tradition_test</th>\n",
       "      <th>Benevolence_test</th>\n",
       "      <th>Universalism_test</th>\n",
       "      <th>Self-Direction_test</th>\n",
       "      <th>Stimulation_test</th>\n",
       "      <th>Hedonism_test</th>\n",
       "      <th>...</th>\n",
       "      <th>Tradition_updated</th>\n",
       "      <th>Benevolence_updated</th>\n",
       "      <th>Universalism_updated</th>\n",
       "      <th>Self-Direction_updated</th>\n",
       "      <th>Stimulation_updated</th>\n",
       "      <th>Hedonism_updated</th>\n",
       "      <th>Achievement_updated</th>\n",
       "      <th>Power_updated</th>\n",
       "      <th>Headers_1_test</th>\n",
       "      <th>Headers_1_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Conformity,Hedonism</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td>Universalism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text-ID  Sentence-ID  Security_test  Conformity_test  Tradition_test  \\\n",
       "0  BG_002            1              0                1               0   \n",
       "1  BG_002            2              0                1               0   \n",
       "2  BG_002            3              0                1               0   \n",
       "3  BG_002            4              0                1               0   \n",
       "4  BG_002            5              0                1               0   \n",
       "\n",
       "   Benevolence_test  Universalism_test  Self-Direction_test  Stimulation_test  \\\n",
       "0                 0                  0                    0                 0   \n",
       "1                 0                  0                    0                 0   \n",
       "2                 0                  0                    0                 0   \n",
       "3                 0                  0                    0                 0   \n",
       "4                 0                  0                    0                 0   \n",
       "\n",
       "   Hedonism_test  ...  Tradition_updated  Benevolence_updated  \\\n",
       "0              0  ...                  0                    0   \n",
       "1              0  ...                  0                    0   \n",
       "2              1  ...                  0                    0   \n",
       "3              0  ...                  0                    0   \n",
       "4              0  ...                  0                    0   \n",
       "\n",
       "   Universalism_updated  Self-Direction_updated  Stimulation_updated  \\\n",
       "0                     0                       0                    0   \n",
       "1                     0                       0                    0   \n",
       "2                     0                       0                    0   \n",
       "3                     0                       0                    0   \n",
       "4                     1                       0                    0   \n",
       "\n",
       "   Hedonism_updated  Achievement_updated  Power_updated       Headers_1_test  \\\n",
       "0                 0                    0              0           Conformity   \n",
       "1                 0                    0              0           Conformity   \n",
       "2                 0                    0              1  Conformity,Hedonism   \n",
       "3                 0                    0              0           Conformity   \n",
       "4                 0                    0              0           Conformity   \n",
       "\n",
       "   Headers_1_updated  \n",
       "0                     \n",
       "1                     \n",
       "2              Power  \n",
       "3                     \n",
       "4       Universalism  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test score and updated output files\n",
    "test_score_path = 'TEST SCORE UPD.csv'  # Update this path if necessary\n",
    "updated_output_path = 'POS_Exact_Match_Output_upd.csv'  # Update this path if necessary\n",
    "\n",
    "test_score_df = pd.read_csv(test_score_path)\n",
    "updated_output_df = pd.read_csv(updated_output_path)\n",
    "\n",
    "# Print the column names to check for discrepancies\n",
    "print(\"Test Score Columns:\")\n",
    "print(test_score_df.columns)\n",
    "\n",
    "print(\"\\nUpdated Output Columns:\")\n",
    "print(updated_output_df.columns)\n",
    "\n",
    "# Define the columns to process (update these if necessary based on the above output)\n",
    "columns_to_process = [\"Security\", \"Conformity\", \"Tradition\", \"Benevolence\", \"Universalism\", \n",
    "                      \"Self-Direction\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power\"]\n",
    "\n",
    "# Merge dataframes based on Text-ID and Sentence-ID\n",
    "merged_df = test_score_df.merge(updated_output_df, on=['Text-ID', 'Sentence-ID'], suffixes=('_test', '_updated'))\n",
    "\n",
    "# Function to find headers with value 1\n",
    "def find_headers_with_one(row, suffix):\n",
    "    return ','.join([col for col in columns_to_process if row[f\"{col}{suffix}\"] == 1])\n",
    "\n",
    "# Add new columns for headers with value 1\n",
    "merged_df['Headers_1_test'] = merged_df.apply(lambda row: find_headers_with_one(row, '_test'), axis=1)\n",
    "merged_df['Headers_1_updated'] = merged_df.apply(lambda row: find_headers_with_one(row, '_updated'), axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "final_columns = ['Text-ID', 'Sentence-ID'] + \\\n",
    "                [f\"{col}_test\" for col in columns_to_process] + \\\n",
    "                [f\"{col}_updated\" for col in columns_to_process] + \\\n",
    "                ['Headers_1_test', 'Headers_1_updated']\n",
    "\n",
    "final_df = merged_df[final_columns]\n",
    "\n",
    "# Save to a new Excel file\n",
    "output_excel_path = 'POS_Exact_MatchCombined_Output.xlsx'  # Update this path if necessary\n",
    "final_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d524f4-ba23-4c1a-8574-3fe45cc715c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  Text-ID  Sentence-ID  Security_test  Conformity_test  Tradition_test  \\\n",
       " 0  BG_002          1.0            0.0              1.0             0.0   \n",
       " 1  BG_002          2.0            0.0              1.0             0.0   \n",
       " 2  BG_002          3.0            0.0              1.0             0.0   \n",
       " 3  BG_002          4.0            0.0              1.0             0.0   \n",
       " 4  BG_002          5.0            0.0              1.0             0.0   \n",
       " \n",
       "    Benevolence_test  Universalism_test  Self-Direction_test  Stimulation_test  \\\n",
       " 0               0.0                0.0                  0.0               0.0   \n",
       " 1               0.0                0.0                  0.0               0.0   \n",
       " 2               0.0                0.0                  0.0               0.0   \n",
       " 3               0.0                0.0                  0.0               0.0   \n",
       " 4               0.0                0.0                  0.0               0.0   \n",
       " \n",
       "    Hedonism_test  ...  Universalism_updated  Self-Direction_updated  \\\n",
       " 0            0.0  ...                   0.0                     0.0   \n",
       " 1            0.0  ...                   0.0                     0.0   \n",
       " 2            1.0  ...                   0.0                     0.0   \n",
       " 3            0.0  ...                   0.0                     0.0   \n",
       " 4            0.0  ...                   1.0                     0.0   \n",
       " \n",
       "    Stimulation_updated  Hedonism_updated  Achievement_updated  Power_updated  \\\n",
       " 0                  0.0               0.0                  0.0            0.0   \n",
       " 1                  0.0               0.0                  0.0            0.0   \n",
       " 2                  0.0               0.0                  0.0            1.0   \n",
       " 3                  0.0               0.0                  0.0            0.0   \n",
       " 4                  0.0               0.0                  0.0            0.0   \n",
       " \n",
       "         Headers_1_test  Headers_1_updated  Test_Match_Percentage  \\\n",
       " 0           Conformity                NaN                    0.0   \n",
       " 1           Conformity                NaN                    0.0   \n",
       " 2  Conformity,Hedonism              Power                    0.0   \n",
       " 3           Conformity                NaN                    0.0   \n",
       " 4           Conformity       Universalism                    0.0   \n",
       " \n",
       "    Updated_Match_Percentage  \n",
       " 0                       0.0  \n",
       " 1                       0.0  \n",
       " 2                       0.0  \n",
       " 3                       0.0  \n",
       " 4                       0.0  \n",
       " \n",
       " [5 rows x 26 columns],\n",
       " 0.3651888974556669,\n",
       " 0.3454396273611184)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined output file\n",
    "combined_output_path = 'POS_Exact_MatchCombined_Output.xlsx'\n",
    "combined_df = pd.read_excel(combined_output_path)\n",
    "\n",
    "# Function to calculate the match percentage\n",
    "def calculate_match_percentage(test_headers, updated_headers):\n",
    "    if pd.isna(test_headers) and pd.isna(updated_headers):\n",
    "        return 1.0, 1.0\n",
    "    \n",
    "    test_words = set(str(test_headers).split(',')) if pd.notna(test_headers) else set()\n",
    "    updated_words = set(str(updated_headers).split(',')) if pd.notna(updated_headers) else set()\n",
    "    \n",
    "    matches = test_words.intersection(updated_words)\n",
    "    num_matches = len(matches)\n",
    "    \n",
    "    test_percentage = num_matches / len(test_words) if test_words else 0.0\n",
    "    updated_percentage = num_matches / len(updated_words) if updated_words else 0.0\n",
    "    \n",
    "    return test_percentage, updated_percentage\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "combined_df[['Test_Match_Percentage', 'Updated_Match_Percentage']] = combined_df.apply(\n",
    "    lambda row: calculate_match_percentage(row['Headers_1_test'], row['Headers_1_updated']), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Calculate the total score for each column\n",
    "total_rows = len(combined_df)\n",
    "test_total_score = combined_df['Test_Match_Percentage'].sum() / total_rows\n",
    "updated_total_score = combined_df['Updated_Match_Percentage'].sum() / total_rows\n",
    "\n",
    "# Add the total score to the dataframe\n",
    "combined_df.loc['Total'] = combined_df.sum(numeric_only=True)\n",
    "combined_df.at['Total', 'Test_Match_Percentage'] = test_total_score\n",
    "combined_df.at['Total', 'Updated_Match_Percentage'] = updated_total_score\n",
    "\n",
    "# Save the updated dataframe to a new Excel file\n",
    "updated_combined_output_path = 'POS_Exact_MatchCombined_fOutput.xlsx'\n",
    "combined_df.to_excel(updated_combined_output_path, index=False)\n",
    "\n",
    "combined_df.head(), test_total_score, updated_total_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
